{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21970afd",
   "metadata": {},
   "source": [
    "# Cross-Dataset Comparison\n",
    "\n",
    "Comparing model performance across different datasets to understand generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2310221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fcc2f2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Dataset | Total Files | Accuracy | Precision | Recall | F1 |\n",
    "|---------|-------------|----------|-----------|--------|----|\n",
    "| Original Eval | ? | 99.7% | - | - | - |\n",
    "| In-The-Wild | ~19k+ | TBD | TBD | TBD | TBD |\n",
    "| Fake-or-Real | ~69k | TBD | TBD | TBD | TBD |\n",
    "\n",
    "**Run both evaluation notebooks first** to populate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f88aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from evaluation notebooks (run them first!)\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "results = []\n",
    "\n",
    "# In-The-Wild results (if available)\n",
    "itw_path = \"../data/results.csv\"  # from notebook 01\n",
    "if os.path.exists(itw_path):\n",
    "    itw_df = pd.read_csv(itw_path)\n",
    "    results.append({\n",
    "        \"dataset\": \"In-The-Wild\",\n",
    "        \"files\": len(itw_df),\n",
    "        \"accuracy\": accuracy_score(itw_df[\"label\"], itw_df[\"pred\"]) * 100,\n",
    "        \"precision\": precision_score(itw_df[\"label\"], itw_df[\"pred\"], pos_label=\"spoof\") * 100,\n",
    "        \"recall\": recall_score(itw_df[\"label\"], itw_df[\"pred\"], pos_label=\"spoof\") * 100,\n",
    "        \"f1\": f1_score(itw_df[\"label\"], itw_df[\"pred\"], pos_label=\"spoof\") * 100\n",
    "    })\n",
    "\n",
    "# Fake-or-Real results (if available)\n",
    "for_path = \"../data/results_fake_or_real.csv\"  # from notebook 02\n",
    "if os.path.exists(for_path):\n",
    "    for_df = pd.read_csv(for_path)\n",
    "    results.append({\n",
    "        \"dataset\": \"Fake-or-Real\",\n",
    "        \"files\": len(for_df),\n",
    "        \"accuracy\": accuracy_score(for_df[\"true\"], for_df[\"pred\"]) * 100,\n",
    "        \"precision\": precision_score(for_df[\"true\"], for_df[\"pred\"], pos_label=\"fake\") * 100,\n",
    "        \"recall\": recall_score(for_df[\"true\"], for_df[\"pred\"], pos_label=\"fake\") * 100,\n",
    "        \"f1\": f1_score(for_df[\"true\"], for_df[\"pred\"], pos_label=\"fake\") * 100\n",
    "    })\n",
    "\n",
    "# Add original eval baseline\n",
    "results.append({\n",
    "    \"dataset\": \"Original Eval (reported)\",\n",
    "    \"files\": None,\n",
    "    \"accuracy\": 99.7,\n",
    "    \"precision\": None,\n",
    "    \"recall\": None,\n",
    "    \"f1\": None\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization comparing datasets\n",
    "if len(comparison_df[comparison_df[\"accuracy\"].notna()]) > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    data = comparison_df[comparison_df[\"accuracy\"].notna()]\n",
    "    colors = [\"green\" if acc > 70 else \"orange\" if acc > 50 else \"red\" for acc in data[\"accuracy\"]]\n",
    "    axes[0].barh(data[\"dataset\"], data[\"accuracy\"], color=colors)\n",
    "    axes[0].axvline(50, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Random\")\n",
    "    axes[0].set_xlabel(\"Accuracy (%)\")\n",
    "    axes[0].set_title(\"Model Accuracy Across Datasets\")\n",
    "    axes[0].set_xlim(0, 100)\n",
    "    \n",
    "    # F1 Score comparison (excluding Original Eval)\n",
    "    f1_data = comparison_df[comparison_df[\"f1\"].notna()]\n",
    "    if len(f1_data) > 0:\n",
    "        axes[1].barh(f1_data[\"dataset\"], f1_data[\"f1\"], color=\"steelblue\")\n",
    "        axes[1].set_xlabel(\"F1 Score (%)\")\n",
    "        axes[1].set_title(\"F1 Score Across Datasets\")\n",
    "        axes[1].set_xlim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Run notebooks 01 and 02 first to generate comparison data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cd1d4",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Questions Answered:\n",
    "\n",
    "1. **Does the model generalize poorly in general, or is In-The-Wild particularly challenging?**\n",
    "   - The model likely struggles with both datasets, indicating a general generalization problem\n",
    "\n",
    "2. **What characteristics make a dataset harder for the model?**\n",
    "   - Different audio sources (YouTube vs TTS engines)\n",
    "   - Different recording conditions and quality\n",
    "   - Different deepfake generation methods than training data\n",
    "\n",
    "3. **Is the issue with the model architecture or training data?**\n",
    "   - Most likely **training data** - the model achieves 99.7% on its original eval set but fails on new data\n",
    "   - This is classic **overfitting to the training distribution**\n",
    "\n",
    "### Recommendations:\n",
    "- Consider fine-tuning on diverse datasets\n",
    "- Use ensemble methods with multiple models\n",
    "- Evaluate on more diverse test sets before deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
