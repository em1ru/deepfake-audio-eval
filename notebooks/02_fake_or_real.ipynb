{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646b555f",
   "metadata": {},
   "source": [
    "# Fake-or-Real Dataset Evaluation\n",
    "\n",
    "Evaluating the same model on the **Fake-or-Real** dataset for comparison.\n",
    "\n",
    "- **Dataset**: [Fake-or-Real](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)  \n",
    "- **Model**: [MelodyMachine/Deepfake-audio-detection-V2](https://huggingface.co/MelodyMachine/Deepfake-audio-detection-V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ceb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../data/fake_or_real\"\n",
    "\n",
    "# TODO: adjust based on dataset structure\n",
    "# fake_files = glob.glob(f\"{DATASET_PATH}/fake/*.wav\")\n",
    "# real_files = glob.glob(f\"{DATASET_PATH}/real/*.wav\")\n",
    "# print(f\"Fake: {len(fake_files)}, Real: {len(real_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c887d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"MelodyMachine/Deepfake-audio-detection-V2\"\n",
    "SAMPLE_RATE = 16_000\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForAudioClassification.from_pretrained(MODEL_ID)\n",
    "model.eval()\n",
    "print(\"Model loaded âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    \"\"\"Run inference on a single audio file.\"\"\"\n",
    "    audio, _ = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
    "    inputs = extractor(audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=-1)[0].numpy()\n",
    "    idx = probs.argmax()\n",
    "    label = model.config.id2label[idx]\n",
    "    \n",
    "    return {\n",
    "        \"file\": os.path.basename(path),\n",
    "        \"pred\": \"fake\" if label.lower() == \"fake\" else \"real\",\n",
    "        \"confidence\": round(probs[idx] * 100, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c03e9",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "TODO: run on dataset sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement inference loop\n",
    "# results = [predict(f) for f in tqdm(sample_files)]\n",
    "# results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef9ad8",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "TODO: calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
